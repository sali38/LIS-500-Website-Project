<!-- tm_emotions.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Emotion Classifier ‚Äì LIS 500 Project 3</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">

  <!-- Teachable Machine / TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@latest/dist/teachablemachine-image.min.js"></script>
</head>

<body>
  <!-- HEADER -->
  <header class="site-header">
    <nav class="navbar container">
      <ul class="navigation">
        <li><a href="index.html">Home</a></li>
        <li><a href="tech_heroes.html">Tech Heroes</a></li>
        <li><a href="resources.html">Resources</a></li>
        <li><a href="tm_emotions.html" aria-current="page" class="active">Emotion ML Demo</a></li>
        <li><a href="about_us.html">About Us</a></li>
      </ul>
    </nav>
  </header>


  <!-- MAIN CONTENT -->
  <main class="container page-content">
    <article class="card">

      <h1 class="page-title">Emotion Classifier ‚Äì Teachable Machine</h1>
      <p>
        This page demonstrates a <strong>webcam-based emotion classifier</strong> trained with Google 
        Teachable Machine. It detects three facial expressions: 
        <b>Happy ¬∑ Sad ¬∑ Angry.</b><br>
        No images or recordings are saved‚Äîeverything runs <strong>locally in your browser</strong>.
      </p>

      <!-- Open Model Button -->
      <div class="center-block" style="margin-top:16px;">
        <a class="hero-btn"
           href="https://teachablemachine.withgoogle.com/models/Bn6RjZdcH/"
           target="_blank" rel="noopener">
          Open Our Teachable Machine Model
        </a>
      </div>

      <!-- HOW TO USE -->
      <section>
        <h2>Try the Demo</h2>
        <p>Click Start, allow camera access, and watch the prediction update in real time!</p>

        <div class="center-block" style="margin:18px 0;">
          <button id="startButton">Start Webcam &amp; Model</button>
          <button id="stopButton">Stop</button>
        </div>

        <div class="center-block">
          <video id="webcam" width="320" height="240" autoplay playsinline
            style="border-radius:10px;border:1px solid #ddd;background:#000;"></video>

          <p style="margin-top:10px;">
            <strong>Prediction:</strong> <span id="predictionLabel">waiting‚Ä¶</span>
          </p>
        </div>

        <p class="source" style="margin-top:12px;">
          *Runs entirely client-side with TensorFlow.js ‚Äî no data sent to a server.
        </p>
      </section>


      <!-- WHY EMOTIONS - HERO CONNECTION -->
      <section>
        <h2>Why Emotions? Connecting to Our Tech Heroes</h2>
        <p>
          Emotion recognition tools already appear in workplaces, classrooms, 
          and surveillance systems. Our Tech Heroes helped us think critically about this:
        </p>

        <ul>
          <li><strong>Ifeoma Ozoma</strong> reminds us that workers need protection against retaliation and unfair scoring.</li>
          <li><strong>Dorothy Vaughan</strong> shows that algorithms mirror the humans who build them ‚Äî not neutral by default.</li>
          <li><strong>Jamie Gong</strong> challenges us to ask whether tech can support care rather than control.</li>
        </ul>
      </section>


      <!-- HOW WE TRAINED -->
      <section>
        <h2>How We Trained the Model</h2>
        <ul>
          <li><strong>Three classes:</strong> happy, sad, angry</li>
          <li>Training data from <strong>all members</strong> (diversity reduces bias)</li>
          <li>Different lighting/backgrounds to improve generalization</li>
          <li>Realistic expressions rather than exaggerated cartoon emotions</li>
        </ul>
        <p>Videos -> image frames -> TensorFlow.js model export -> integrated into this webpage.</p>
      </section>


      <!-- COLLAPSIBLE PROJECT STATEMENT -->
      <section>
        <h2>Project Statement</h2>

        <details>
          <summary style="cursor:pointer;font-weight:600;">Click to read our full project statement</summary>
          <div style="margin-top:12px;">
            <p>
              Our project draws inspiration from <strong>Joy Buolamwini‚Äôs "Unmasking AI"</strong>, emphasizing the biases embedded within facial recognition technologies. Buolamwini's work demonstrates how systems fail disproportionately on darker-skinned women ‚Äî a reminder that <strong>data is political</strong>, not neutral.
            </p>
            <p>
              As a team, we reflected on how systems misrepresent marginalized identities. Samira highlighted misidentification of historical Black women, Yunji emphasized the right to biometric privacy, and Ellah focused on structural bias within early AI systems.
            </p>
            <p>
              Our ML demo uses three emotions ‚Äî happiness, sadness, and anger ‚Äî chosen for universality while recognizing they appear differently across people and cultures. We used <strong>multiple faces, angles, and conditions</strong> to avoid overfitting to one identity.
            </p>
            <p>
              This project is small-scale, but it represents our commitment to equitable, socially informed AI design. Technology should serve people ‚Äî not the other way around.
            </p>
          </div>
        </details>
      </section>


      <!-- LIMITATIONS -->
      <section>
        <h2>Limitations & Open Questions</h2>
        <ul>
          <li>Best performance on the three creators; may fail on unfamiliar faces</li>
          <li>Lighting and angle strongly affect predictions</li>
          <li>Complex or subtle expressions are difficult to classify</li>
        </ul>
        <p>
          The goal is not accuracy, but <strong>critical reflection</strong> on how 
          emotion recognition technology is used ‚Äî 
          and misused ‚Äî in real institutions.
        </p>
      </section>

    </article>
  </main>


  <!-- FOOTER-->
  <footer class="footer">
    <p>Made by LIS500 Project Group 1üçà</p>
  </footer>


  <!-- JavaScript connecting the model -->
  <script src="tm_emotion.js"></script>
</body>
</html>
